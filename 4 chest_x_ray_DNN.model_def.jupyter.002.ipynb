{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHEST X-RAY IMAGE CLASSIFICATION ADVANCED DATA SCIENCE CAPSTONE PROJECT**\n",
    "\n",
    "Paolo Cavadini, February 2021.\n",
    "\n",
    "Dataset\n",
    "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find on GitHub: https://github.com/pcavad/capstone_x_rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from adsMod import plot_confusion_matrix, modeling #my module for this project\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, MaxPool2D \n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, SeparableConv2D \n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint,EarlyStopping \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./data/X_train.npy')\n",
    "y_train = np.load('./data/y_train.npy')\n",
    "X_test = np.load('./data/X_test.npy')\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics regression (non-deep learning model as baseline)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-shaping the data to fit for a logistics regression.\n",
    "\n",
    "X_train_LR = np.reshape(X_train, (len(X_train),-1))\n",
    "X_test_LR = np.reshape(X_test, (len(X_test),-1))\n",
    "y_train_LR = np.reshape(y_train, (len(y_train),))\n",
    "y_test_LR = np.reshape(y_test, (len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and predicting using normal parameters.\n",
    "\n",
    "LR = LogisticRegression(C=0.1, solver='liblinear').fit(X_train_LR,y_train_LR)\n",
    "yhat = LR.predict(X_test_LR)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the sci-kit learn accuracy score which I'll use as baseline to improve the neural network.\n",
    "\n",
    "print('Accuracy score: {:.2f}%'.format(accuracy_score(y_test_LR, yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix.\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "# Plot confusion matrix, function from adsMod\n",
    "plot_confusion_matrix(cnf_matrix, classes=['pneumonia','normal'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEEP LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the input shape and a batch size of 32 which is a good compromise between accuracy and response time. The activation function and the optimizer will be search trhough a fine tuning process as well as the option to augment the data. I chose the binary crossentropy loss function because it is a common choice for classification, and the accuracy metric because it is easy to interpret.\n",
    "\n",
    "I will compose a neural network in 5 convolutional blocks with convolutional layer, max-pooling and batch-normalization. On top of it you find a flatten layer followed by two dense layers, and in between a dropout layer to reduce over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (150,150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just running a test\n",
    "model = modeling('relu', 'adam', input_shape) #function from adsMod\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
