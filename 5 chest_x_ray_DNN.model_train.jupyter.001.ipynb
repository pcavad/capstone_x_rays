{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHEST X-RAY IMAGE CLASSIFICATION ADVANCED DATA SCIENCE CAPSTONE PROJECT**\n",
    "\n",
    "Paolo Cavadini, February 2021.\n",
    "\n",
    "Dataset\n",
    "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find on GitHub: https://github.com/pcavad/capstone_x_rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, MaxPool2D \n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, SeparableConv2D \n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint,EarlyStopping \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./data/X_train.npy')\n",
    "y_train = np.load('./data/y_train.npy')\n",
    "X_test = np.load('./data/X_test.npy')\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (150,150,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(activation_f=None, optimizer_f=None, input_shape_img=None):\n",
    "    '''\n",
    "    This function creates the model based on different parameters, complile and save it.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    #1st block\n",
    "    model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = activation_f , input_shape = input_shape_img))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    #2nd block\n",
    "    model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = activation_f))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    #3rd block\n",
    "    model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = activation_f))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    #4th block\n",
    "    model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = activation_f))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    #5th block\n",
    "    model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = activation_f))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 128 , activation = activation_f))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 1 , activation = 'sigmoid')) \n",
    "\n",
    "    model.compile(optimizer=optimizer_f, loss='binary_crossentropy', metrics=['accuracy']) # compiling ##'MeanSquaredError'\n",
    "    \n",
    "    model.save('./data/model.h5') # saving\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine tuning optimizer and activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through different optimizers and activation functions to find the best score.\n",
    "input_shape_img = (150,150,3)\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "optimizers = ['adam', 'rmsprop']\n",
    "# best_score = 0\n",
    "best_accuracy = 0\n",
    "best_optimizer = None\n",
    "best_activation_function = None\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    for optimizer_function in optimizers:\n",
    "        modeling(activation_function, optimizer_function, input_shape) # model function\n",
    "        model = load_model('./data/model.h5') # load model\n",
    "        history = model.fit( \n",
    "                            X_train, y_train,\n",
    "                            batch_size=32, #1 epoch (default)\n",
    "                            validation_split=0.2,\n",
    "                            verbose=1) # validation set\n",
    "        if history.history['accuracy'][0] > best_accuracy:\n",
    "            best_accuracy = history.history['accuracy'][0]\n",
    "            best_optimizer = optimizer_function\n",
    "            best_activation_function = activation_function\n",
    "            model.save('./data/best_model.h5')\n",
    "\n",
    "print('The best optimizer is {}, the best activation function is {}, for an accuracy of {:.2f}%'\n",
    "     .format(optimizer_function, best_activation_function, best_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have estimated the best activation function and the best optimizer, which we'll use in the next fine tuning steps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINE TUNING FEATURES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the proportion between scans with and without pneumonia, and calculating the weights to counter-balance the bias during training.\n",
    "\n",
    "count_normal = len(y_train[y_train==0]) \n",
    "count_pneumonia = len(y_train[y_train==1])\n",
    "initial_bias = np.log([count_pneumonia / count_normal]) \n",
    "print(\"Initial bias: {:.4f}\".format(initial_bias[0]))\n",
    "\n",
    "train_img_count = len(y_train)\n",
    "weight_for_0 = (1 / count_normal) * train_img_count / 2.0\n",
    "weight_for_1 = (1 / count_pneumonia) * train_img_count / 2.0\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
    "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model adding class weights\n",
    "\n",
    "model = load_model('./data/best_model.h5')\n",
    "history = model.fit(\n",
    "                    X_train,y_train,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    class_weight=class_weights,\n",
    "                    verbose=1) # 1 epoch (default)\n",
    "if history.history['accuracy'][0] > best_accuracy:\n",
    "    best_accuracy = history.history['accuracy'][0]\n",
    "    print('Accuracy improved: ', best_accuracy*100)\n",
    "else:\n",
    "    print('Adding class weights didn\\'t improve the accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like adding the weights improved our model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING THE FINALLY OPTIMIZED MODEL (10 epochs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about performance:** becasue training over different epochs may become too intensive for my hardware I run the model under IBM Watson Studio where I can setup a Notebook with a proper choice of runtime (more CPUs, more memory). To implement the Notebook I just need to upload the X and Y arrays as file in the Object Cloud store and use them instead of parsing the images with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation means increasing the samples by means of adding scans from different perspectives, which bring the model closer to reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Callbacks**\n",
    "\n",
    "Before training the model is useful to define one or more callbacks:\n",
    "\n",
    "1. ModelCheckpoint: save a copy of the best performing model when an epoch that improves the metrics ends.\n",
    "\n",
    "2. EarlyStopping: stop training when the difference between training and validation error starts to increase, instead of decreasing (overfitting).\n",
    "\n",
    "Optimizing the learning rate:\n",
    "\n",
    "Learning rate is a descent step which the optimizing algorithms take in order to converge to a local optimum. The learning rate should not be too high to take very large steps nor it should be too small which would not alter the weights and biases. The ReduceLRonPlateau monitors the learning rate and if no improvement is seen for a (patience) number of epochs then the learning rate is reduced by a factor specified as one of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing callbacks\n",
    "path = f'./data/model.h5'\n",
    "\n",
    "# Saves the model in-between epochs when there is an improvement in val_loss\n",
    "checkpoint = ModelCheckpoint(path,\n",
    "                                monitor='val_loss',\n",
    "                                mode=\"min\",\n",
    "                                save_best_only = True,\n",
    "                                verbose=1)\n",
    "\n",
    "# Stops training the model when no improvement in val_loss is observed after set patience\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                              min_delta = 0, \n",
    "                              patience = 4,\n",
    "                              verbose = 1,\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "# Monitors val_accuracy for a set 'patience', then the learning rate is reduced by a factor specified in the parameters\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', ## val_accuracy\n",
    "                              patience = 2,\n",
    "                              verbose=1,\n",
    "                              factor=0.3, # reduction\n",
    "                              min_lr=0.000001)\n",
    "\n",
    "# callbacks pipeline\n",
    "callbacks_pipeline = [checkpoint, earlystop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling and training using the chosen optimizer, activation function, callbacks and no data augmentation.\n",
    "filt = np.random.rand(len(X_train))\n",
    "\n",
    "model = load_model('./data/best_model.h5')\n",
    "history = model.fit(\n",
    "#                 X_train,y_train,\n",
    "                datagen.flow(X_train[filt < 0.8],y_train[filt < 0.8], batch_size = 32),\n",
    "                batch_size=32,\n",
    "#                 validation_split=0.2,\n",
    "                class_weight=class_weights,\n",
    "                validation_data=datagen.flow(X_train[filt > 0.8], y_train[filt > 0.8]), \n",
    "                epochs=10, #10 epochs\n",
    "                callbacks=callbacks_pipeline,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting and saving\n",
    "preds = model.predict(X_test)\n",
    "np.save('./data/preds.npy',preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving history\n",
    "df_history=pd.DataFrame(history.history)\n",
    "df_history.to_csv('./data/training_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
